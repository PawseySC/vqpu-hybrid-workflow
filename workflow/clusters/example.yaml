# generic slurm job
generic:
  cluster_class: "dask_jobqueue.SLURMCluster"
  cluster_kwargs:
    name: generic

    # Dask worker options
    cores: 1                 # Total number of cores per job
    memory: "1GB"                # Total amount of memory per job
    processes: 1                # Number of Python processes per job

    # python: python3                # Python executable
    death_timeout: 30           # Number of seconds to wait if a worker can not find a scheduler
    local_directory: "$TMPDIR"       # Location of fast local storage like /scratch or $TMPDIR
    shared_temp_directory: null       # Shared directory currently used to dump temporary security objects for workers
    interface: "" # try running workflow to see what are allowed interfaces

    # SLURM resource manager options
    shebang: "#!/usr/bin/bash"
    queue: ""
    account: ""
    walltime: "1:00:00"
    # env-extra: null
    job_script_prologue: 
      - "export OMP_NUM_THREADS=1"
      - "export OMP_PLACES=cores"
      - "export OMP_MAX_ACTIVE_LEVELS=4"
      - "echo \"Node state:\""
      - "hostname"
      - "python --version"
      - "prefect version"
      - "env | grep SLURM_"
    job_cpu: 1
    job_mem: "1GB"
    job_extra: null
    # add anything extra #SBATCH additions
    job_extra_directives: 
      - "--ntasks=1"
      - '-e dask_logs/dask_generic_worker_%J.err'
      - '-o dask_logs/dask_generic_worker_%J.log'
    log_directory: dask_logs
    silence_logs: 'info'
    job_directives_skip: ['-e', '-o']    

  # now for number of slurm jobs
  adapt_kwargs:
      minimum_jobs: 1
      maximum_jobs: 70
      minimum_workers: 1
      maximum_workers: 144
      target_duration: "30s" # Amount of time we want a computation to take. This affects how aggressively we scale up.
      interval: "1s" # Milliseconds between checks
      wait_count: 1 # Number of consecutive times that a worker should be suggested for removal before we remove it.

# for running circuits
circuit:
  cluster_class: "dask_jobqueue.SLURMCluster"
  cluster_kwargs:
    name: circuit

    # copy above entries and alter as needed

#virtual qpu slurm configuration
vqpu:
  cluster_class: "dask_jobqueue.SLURMCluster"
  cluster_kwargs:
    name: vqpu

    # copy above entries and alter as needed


#gpu slurm configuration
gpu:
  cluster_class: "dask_jobqueue.SLURMCluster"
  cluster_kwargs:
    name: gpu
    # copy above entries and alter as needed

#cpu slurm configuration
cpu:
  cluster_class: "dask_jobqueue.SLURMCluster"
  cluster_kwargs:
    name: cpu

distributed:
  worker:
    memory:
      target: False    # Avoid spilling to disk
      spill: False     # Avoid spilling to disk
